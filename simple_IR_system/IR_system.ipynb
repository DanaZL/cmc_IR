{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib\n",
    "import os\n",
    "import pymorphy2\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "RusLem = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def bprint(l, sep = \" \"):\n",
    "    print sep.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    " \n",
    "\n",
    "#get list of sentences from html pages\n",
    "def prepare_html_pages(html_dir):\n",
    "    html_files = os.listdir(html_dir)\n",
    "    sentences = []\n",
    "    for filename in html_files:\n",
    "        text = \"\" \n",
    "        with open(html_dir + filename, \"r\") as f:\n",
    "            for line in f:\n",
    "                text += line\n",
    "                \n",
    "        visible_text = text_from_html(text)\n",
    "        sentences += visible_text.strip().split(\".\")\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "#sentences_normaliation\n",
    "def normalization(sentences):\n",
    "    for i, sent in enumerate(sentences):\n",
    "        tokens = re.findall('[\\w]+', sent.strip().lower(), re.U)\n",
    "        sentences[i] = \" \".join([RusLem.parse(token)[0].normal_form for token in tokens])\n",
    "    return sentences\n",
    "\n",
    "\n",
    "#creating forward index {sent_id: list of word} and token_dictionary\n",
    "def create_forward_index(sentences):\n",
    "    forward_index = {}\n",
    "    token_list = set()\n",
    "    for sent_id, sent in enumerate(sentences):\n",
    "        if 1 < len(sent.strip().split()) <= 40:\n",
    "            forward_index[sent_id] = sent.strip().split()\n",
    "            for token in sent.strip().split():\n",
    "                token_list.add(token)\n",
    "        \n",
    "    return forward_index, list(token_list)\n",
    "\n",
    "\n",
    "def calculate_idf(forward_index, token_list):\n",
    "    cnt_sentences = len(forward_index)\n",
    "    token_df = {token:0 for token in token_list}\n",
    "    for tokens in forward_index.values():\n",
    "        for token in tokens:\n",
    "            token_df[token] += 1\n",
    "    token_idf = {token:np.log(cnt_sentences / float(token_df[token])) for token in token_df}\n",
    "    return token_idf\n",
    "\n",
    "def vec_normalization(vector):\n",
    "    return vector / (1.0 * np.sqrt(np.sum(vector**2)))\n",
    "\n",
    "def cos_similarity(vec_1, vec_2):\n",
    "    return np.sum(vec_1 * vec_2) / float(np.sqrt(np.sum(vec_1**2)) * np.sqrt(np.sum(vec_2**2)))\n",
    "\n",
    "def get_tokens_probs(forward_index, token_list):\n",
    "    tokens_cnt = 0\n",
    "    token_probs = {token:0 for token in token_list}\n",
    "    for sent_idx in forward_index:\n",
    "        for token in forward_index[sent_idx]:\n",
    "            token_probs[token] += 1\n",
    "        tokens_cnt += len(forward_index[sent_idx])\n",
    "        \n",
    "    token_probs = {token:token_probs[token] / float(tokens_cnt) for token in token_list}\n",
    "    return token_probs\n",
    "        \n",
    "    \n",
    "class Collection(object):\n",
    "    def __init__(self, htmls_dir=\"./htmls/\"):\n",
    "        self.raw_sentences = prepare_html_pages(htmls_dir)\n",
    "        print len(self.raw_sentences)\n",
    "        self.norm_sentences = normalization(deepcopy(self.raw_sentences))\n",
    "        print len(self.norm_sentences)\n",
    "        self.forward_index, self.token_list = create_forward_index(self.norm_sentences)\n",
    "        print len(self.forward_index)\n",
    "        self.token_idf = calculate_idf(self.forward_index, self.token_list)\n",
    "        self.token_prob = get_tokens_probs(self.forward_index, self.token_list)\n",
    "        \n",
    "    def doc2vec(self, sentence, idf=False):\n",
    "        \"\"\"\n",
    "        Create sentence normalized vector\n",
    "        \"\"\"\n",
    "        vector = [sentence.count(token) for token in self.token_list]\n",
    "#         print sum(vector)\n",
    "        if idf:\n",
    "            vector = [c * np.log(self.token_idf[self.token_list[i]])\n",
    "                      for i, c in enumerate(vector)]\n",
    "            \n",
    "        return np.asarray(vector)\n",
    "        \n",
    "    def sents2vectors(self):\n",
    "        self.simple_vectors = {sent_idx:vec_normalization(\n",
    "                                        self.doc2vec(self.forward_index[sent_idx], idf=False)\n",
    "                                        )\n",
    "                               for sent_idx in self.forward_index.keys()}\n",
    "        \n",
    "        self.idf_vectors = {sent_idx:vec_normalization(\n",
    "                                    self.doc2vec(self.forward_index[sent_idx], idf=True)\n",
    "                                    )\n",
    "                               for sent_idx in self.forward_index.keys()}\n",
    "        \n",
    "    def vec_ranking(self, query):\n",
    "        norm_query = normalization([query])[0]\n",
    "        bprint([norm_query])\n",
    "        query_vec_simple = self.doc2vec(norm_query.strip().split(), idf=False)\n",
    "        query_vec_idf = self.doc2vec(norm_query.strip().split(), idf=True)\n",
    "\n",
    "        rank_simple = {}\n",
    "        rank_tfidf = {}\n",
    "        for sent_idx in self.simple_vectors.keys():\n",
    "            rank_simple[sent_idx] = cos_similarity(query_vec_simple,\n",
    "                                                   self.simple_vectors[sent_idx])\n",
    "            rank_tfidf[sent_idx] = cos_similarity(query_vec_idf,\n",
    "                                                   self.idf_vectors[sent_idx])\n",
    "        \n",
    "        return rank_simple, rank_tfidf\n",
    "    \n",
    "    #lambda - l\n",
    "    def lang_model_ranking(self, query, l=0.5):\n",
    "        norm_query = normalization([query])[0]\n",
    "        rank = {}\n",
    "        for sent_idx in forward_index:\n",
    "            token_query_prob = sum([1 if token in norm_query else 0 for token in forward_index[sent_idx]])\n",
    "            rank[sent_idx] = [(1 - l) * self.token_prob(token) + l * token_query_prob for token in norm_query]\n",
    "            \n",
    "\n",
    "def print_serp(rank, query, sent_cnt = 10):\n",
    "    sorted_rank = sorted(rank.items(), key = operator.itemgetter(1), reverse=True)\n",
    "    print \"Query:\",\n",
    "    bprint([query])\n",
    "    print \"\\n\"\n",
    "    serp = []\n",
    "    \n",
    "    for idx, pair in enumerate(sorted_rank):\n",
    "#         print idx, pair\n",
    "        if idx > sent_cnt:\n",
    "            break\n",
    "        print pair[1], \"\\t\", pair[0], \n",
    "        bprint(collection.forward_index[pair[0]])\n",
    "        print\n",
    "        serp.append(pair[0])\n",
    "    return serp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1169\n",
      "1169\n",
      "933\n"
     ]
    }
   ],
   "source": [
    "collection = Collection()\n",
    "collection.sents2vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = u\"В начале XX века немецкий учитель математики, сам того не желая, научил лошадь считать.\"\n",
    "query_2 = u\"Искусственный язык для фантастической вселенной «Звёздный путь» создал профессиональный лингвист\"\n",
    "query_3 = u\"На территории России были обнаружены останки ниппонозаврa, амурозаврa,целурозаврa и другие виды динозавров.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в начало xx век немецкий учитель математика сам тот не желать научить лошадь считать\n",
      "искусственный язык для фантастический вселенная звёздный путь создать профессиональный лингвист\n",
      "на территория россия быть обнаружить останки ниппонозаврa амурозаврa целурозаврa и другой вид динозавр\n"
     ]
    }
   ],
   "source": [
    "rank_simple_1, rank_idf_1 = collection.ranking(query=query_1)\n",
    "rank_simple_2, rank_idf_2 = collection.ranking(query=query_2)\n",
    "rank_simple_3, rank_idf_3 = collection.ranking(query=query_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: В начале XX века немецкий учитель математики, сам того не желая, научил лошадь считать.\n",
      "\n",
      "\n",
      "0.284747398726 \t111 в начало век популярный в сша стать блюз и джаз который сохранять свой господство в музыка до появление рок н ролл в 1950 х год\n",
      "\n",
      "0.283069258536 \t71 содержание скрыть 1 основной событие 2 главный изобретение 3 использование сочетание xx век в название 4 двадцатый век в искусство 5 смотреть\n",
      "\n",
      "0.283069258536 \t205 также править править вика текст в викитека есть текст по тема документ xx век xx век хронология изобретение\n",
      "\n",
      "0.283069258536 \t1102 xxii век править править вика текст в начало xxii век в клингонский общество повсеместно усилиться влияние класс воин\n",
      "\n",
      "0.277350098113 \t824 в многий страна в тот число и в россия в 21 век наблюдаться тенденция к снижение престижность педагогический профессия и как следствие недооценённость учительский труд\n",
      "\n",
      "0.273861278753 \t197 в сша один из крупный киностудия называться xx век фокс\n",
      "\n",
      "0.258198889747 \t838 учитель в образовательный процесс править править вика текст монумент учитель и ученик в росток\n",
      "\n",
      "0.258198889747 \t859 компонент педагогический культура учитель в\n",
      "\n",
      "0.25515518154 \t156 тем не менее в конец хх век инфекционный болезнь впервые в история человечество уступить первенство в качество причина смерть заболевание сердечно сосудистый система и злокачественный новообразование\n",
      "\n",
      "0.246182981959 \t835 тем не менее в результат реформа средний заработный плата учитель в москва с 2010 по 2014 год вырасти на 79\n",
      "\n",
      "0.240771706172 \t115 визуальный культура стать доминировать не только в кино и телевидение но проникнуть в литература в вид комикс\n",
      "\n"
     ]
    }
   ],
   "source": [
    "serp_simple_1 = print_serp(rank_simple_1, query_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: В начале XX века немецкий учитель математики, сам того не желая, научил лошадь считать.\n",
      "\n",
      "\n",
      "0.30095035986 \t549 ганс считать\n",
      "\n",
      "0.242162149102 \t561 другой слово ганс быть действительно феноменально умный лошадь и прекрасно понимать что от он хотеть но конечно ни математика ни немецкий язык он не знать и не понимать\n",
      "\n",
      "0.217894923247 \t205 также править править вика текст в викитека есть текст по тема документ xx век xx век хронология изобретение\n",
      "\n",
      "0.185836111712 \t544 комиссия возглавить философ и психолог карл штумпф в состав её войти самый разный человек чей профессия быть так или иначе связать с лошадь математика или психология врач ветеринар владелец цирк офицер кавалерия несколько школьный учитель математика и директор берлинский зоопарк\n",
      "\n",
      "0.16384848594 \t198 название популярный советский боевик пират xx век\n",
      "\n",
      "0.151959371158 \t189 начало эпидемия спид\n",
      "\n",
      "0.149983079329 \t197 в сша один из крупный киностудия называться xx век фокс\n",
      "\n",
      "0.147979632671 \t607 некоторый считать что дромеозаврид группа манираптор хороший классифицировать как птица а не динозавр 2\n",
      "\n",
      "0.144037633628 \t899 сам себя задавать темп работа\n",
      "\n",
      "0.14387317272 \t196 в советский союз и рф до 1995 год выходить журнал век xx и мир\n",
      "\n",
      "0.143677419104 \t126 сам применение вычислительный техника в второй половина хх век изменить характер математический вычисление заставить математик отказаться от метод классический математический анализ и перейти к метод дискретный прикладной математика\n",
      "\n"
     ]
    }
   ],
   "source": [
    "serp_idf_1 = print_serp(rank_idf_1, query_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Искусственный язык для фантастической вселенной «Звёздный путь» создал профессиональный лингвист\n",
      "\n",
      "\n",
      "0.36514837167 \t234 звёздный путь англ\n",
      "\n",
      "0.363912671437 \t371 он известный как сценарист другой сериал в вселенная звёздный путь звёздный путь глубокий космос 9 1993 1999 звёздный путь в яджер 1995 2001\n",
      "\n",
      "0.350823207723 \t255 вселенная звёздный путь один из наиболее детально проработать вымышленный вселенная 1\n",
      "\n",
      "0.34749779421 \t374 он являться режиссёр и сценарист три фильм из серия звёздный путь звёздный путь ii гнев хан 1982 звёздный путь iv путешествие домой 1986 звёздный путь vi неоткрытый страна 1991\n",
      "\n",
      "0.338061701891 \t356 основный статья звёздный путь энтерпрайза звёздный путь энтерпра йз англ\n",
      "\n",
      "0.335410196625 \t1137 он создать специально для фильм лингвист марк окранд\n",
      "\n",
      "0.316227766017 \t4 2 звёздный путь 1\n",
      "\n",
      "0.310086836473 \t1 marc okrand mɑrk ˈoʊkrænd 3 июль 1948 американский лингвист известный как создатель язык для народ фантастический мир кинематограф сша клингонский вулканский и атлантский язык\n",
      "\n",
      "0.282842712475 \t434 звёздный путь исполниться 40 англ\n",
      "\n",
      "0.282842712475 \t1068 klingons ˈ k l ɪ ŋ ɒ n вымышленный инопланетный цивилизация гуманоид воин из научно фантастический вселенная звёздный путь 1\n",
      "\n",
      "0.271163072273 \t238 автор идея и основатель вселенная звёздный путь ведущий свой начало с выход на экран 8 сентябрь 1966 год сериал звёздный путь оригинальный сериал являться джина родденберри 1 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "serp_simple_2 = print_serp(rank_simple_2, query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Искусственный язык для фантастической вселенной «Звёздный путь» создал профессиональный лингвист\n",
      "\n",
      "\n",
      "0.349282738206 \t1137 он создать специально для фильм лингвист марк окранд\n",
      "\n",
      "0.288200424825 \t1 marc okrand mɑrk ˈoʊkrænd 3 июль 1948 американский лингвист известный как создатель язык для народ фантастический мир кинематограф сша клингонский вулканский и атлантский язык\n",
      "\n",
      "0.243760095689 \t255 вселенная звёздный путь один из наиболее детально проработать вымышленный вселенная 1\n",
      "\n",
      "0.222160411704 \t371 он известный как сценарист другой сериал в вселенная звёздный путь звёздный путь глубокий космос 9 1993 1999 звёздный путь в яджер 1995 2001\n",
      "\n",
      "0.215633043521 \t937 можно выделить следующий уровень профессиональный подготовка педагог существующий в настоящее время уровень профессиональный ориентация педагогический класс школа среднее профессиональный образование педагогический колледж высокий профессиональный образование высокий учебный заведение подготовка научно педагогический кадр для высокий профессиональный образование аспирантура докторантура\n",
      "\n",
      "0.208211762035 \t234 звёздный путь англ\n",
      "\n",
      "0.195002916722 \t1068 klingons ˈ k l ɪ ŋ ɒ n вымышленный инопланетный цивилизация гуманоид воин из научно фантастический вселенная звёздный путь 1\n",
      "\n",
      "0.191623322655 \t4 2 звёздный путь 1\n",
      "\n",
      "0.16219116977 \t238 автор идея и основатель вселенная звёздный путь ведущий свой начало с выход на экран 8 сентябрь 1966 год сериал звёздный путь оригинальный сериал являться джина родденберри 1 2\n",
      "\n",
      "0.161542086064 \t260 раса править править вика текст основный статья раса star trek вселенная звёздный путь англ\n",
      "\n",
      "0.159298619224 \t374 он являться режиссёр и сценарист три фильм из серия звёздный путь звёздный путь ii гнев хан 1982 звёздный путь iv путешествие домой 1986 звёздный путь vi неоткрытый страна 1991\n",
      "\n"
     ]
    }
   ],
   "source": [
    "serp_idf_2 = print_serp(rank_idf_2, query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: На территории России были обнаружены останки ниппонозаврa, амурозаврa,целурозаврa и другие виды динозавров.\n",
      "\n",
      "\n",
      "0.362738125055 \t467 первый и единственный неполный скелет быть обнаружить японец в 1934 год на территория больница в синегорск сахалин\n",
      "\n",
      "0.3 \t886 уметь слушать и слышать другой и иной мнение\n",
      "\n",
      "0.283980917124 \t141 транспорт тысячелетие основать на конный тяга быть на протяжение хх век заменить на грузовой автомобиль и автобус что стать возможный благодаря крупномасштабный эксплуатация ископаемое топливо\n",
      "\n",
      "0.282842712475 \t839 скульптура учитель на территория мади\n",
      "\n",
      "0.279751442472 \t529 среди вопрос на который он давать ответ быть не только такой как сколько быть 12 12 но и например если восьмой день месяц приходиться на вторник то какой день по счёт быть следующий пятница\n",
      "\n",
      "0.273861278753 \t20 для клингонский важно быть быть непохожий на привычный земной язык\n",
      "\n",
      "0.258198889747 \t521 результат исследование пфунгст быть принять научный сообщество и использоваться в эксперимент по интеллект животное и человек чтобы избежать влияние экспериментатор на испытуемый\n",
      "\n",
      "0.258198889747 \t605 орнитомимозавр немалоизвестный похожий на страус динозавр\n",
      "\n",
      "0.258198889747 \t892 брать на себя обязательство и ответственность\n",
      "\n",
      "0.253546276419 \t145 беспилотный космический зонд стать практический и относительно недорогой вид разведка и телекоммуникация\n",
      "\n",
      "0.253546276419 \t598 другой примитивный целурозавр орнитолестёс и целура последний изобразить на иллюстрация жить немного поздний процератозавр\n",
      "\n"
     ]
    }
   ],
   "source": [
    "serp_simple_3 = print_serp(rank_simple_3, query_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: На территории России были обнаружены останки ниппонозаврa, амурозаврa,целурозаврa и другие виды динозавров.\n",
      "\n",
      "\n",
      "0.286927874807 \t467 первый и единственный неполный скелет быть обнаружить японец в 1934 год на территория больница в синегорск сахалин\n",
      "\n",
      "0.24572800686 \t839 скульптура учитель на территория мади\n",
      "\n",
      "0.21050651436 \t659 единственный известный вид amurosaurus riabinini назвать в честь покойный палеонтолог анатолий рябинин который возглавить первый русский экспедиция 1916 и 1917 год для поиск окаменелый останки динозавр 4 5\n",
      "\n",
      "0.159383066239 \t885 помогать другой\n",
      "\n",
      "0.149710852665 \t605 орнитомимозавр немалоизвестный похожий на страус динозавр\n",
      "\n",
      "0.148047065197 \t155 пандемия а в конец век быть обнаружить новое вирусный заболевание спид который возникнуть в африка\n",
      "\n",
      "0.147064411445 \t656 amurosaurus родиться птицетазовый динозавр из подсемейство ламбеозаврин жить в конец меловой период верхний маастрихта 2 найти в россия в благовещенск 3\n",
      "\n",
      "0.143146067901 \t664 быть вскрыть лишь небольшой часть слой но 90 найти останки принадлежать ламбеозаврина такой как амурозавр в основное подросток остальной принадлежать другой таксон например гадрозаврид kerberosaurus\n",
      "\n",
      "0.141604093381 \t771 6 вид индивидуальный стиль педагогический деятельность 2\n",
      "\n",
      "0.133993126432 \t231 о другой значение смотреть\n",
      "\n",
      "0.1246960871 \t634 о четырёхкрылый динозавр и происхождение птица природа\n",
      "\n"
     ]
    }
   ],
   "source": [
    "serp_idf_3 = print_serp(rank_idf_3, query_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Размечаю истинную релевантность предложений для 1-го запроса: {doc_id:relevance}\n",
    "true_relevance_1 = {111:0, 71:0, 205:0, 1102:0, 824:0, 197:0, 838:0, 859:0, 156:0, 835:0, 115:0,\n",
    "                     549:1, 561:2, 205:0, 544:1, 198:0, 189:0, 607:0, 899:0, 196:0, 126:0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Размечаю истинную релевантность предложений для 2-го запроса: {doc_id:relevance}\n",
    "true_relevance_2 = {234:1, 371:1, 255:1, 374:1, 356:1, 1137:2, 4:1, 1:2, 434:1, 1068:2, 238:1,\n",
    "                   937:0, 260:1}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Размечаю истинную релевантность предложений для 3-го запроса: {doc_id:relevance}\n",
    "true_relevance_3 = {467:1, 886:0, 141:0, 839:0, 529:0, 20:0, 521:0, 605:1, 892:1, 145:0, 598:1,\n",
    "                   659:2, 885:0, 155:0, 656:2, 664:2, 771:0, 231:0, 634:1}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ndcg(y_true, y_score, N = 10):\n",
    "#     y_true = y_true.ravel()\n",
    "#     y_score = y_score.ravel()\n",
    "    y_true_sorted = sorted(y_true, reverse=True)\n",
    "    ideal_dcg = 0\n",
    "    for i in range(N):\n",
    "        ideal_dcg += (2 ** y_true_sorted[i] - 1.) / np.log2(i + 2)\n",
    "        \n",
    "    dcg = 0\n",
    "    argsort_indices = np.argsort(y_score)[::-1]\n",
    "    for i in range(N):\n",
    "        dcg += (2 ** y_true[argsort_indices[i]] - 1.) / np.log2(i + 2)\n",
    "    if ideal_dcg == 0:\n",
    "        return 1\n",
    "    ndcg = dcg / ideal_dcg\n",
    "    return ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ideal_dcg(marks, N=10):\n",
    "    marks_sorted = sorted(marks, reverse=True)\n",
    "    ideal_dcg = 0\n",
    "    for i in range(N):\n",
    "        ideal_dcg += (1.0 * marks_sorted[i]) / np.log2(i + 2)\n",
    "    return ideal_dcg\n",
    "        \n",
    "def ndcg(marks, ideal_dcg, N=10):\n",
    "    dcg = 0\n",
    "    for i in range(N):\n",
    "        dcg += (1.0 * marks[i]) / np.log2(i + 2)\n",
    "    if ideal_dcg == 0:\n",
    "        return 1\n",
    "    \n",
    "    ndcg = dcg / float(ideal_dcg)\n",
    "    return ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rank_true_simple_1 = [true_relevance_1[doc_id] for doc_id in serp_simple_1]\n",
    "rank_true_idf_1 = [true_relevance_1[doc_id] for doc_id in serp_idf_1]\n",
    "rank_true_simple_2 = [true_relevance_2[doc_id] for doc_id in serp_simple_2]\n",
    "rank_true_idf_2 = [true_relevance_2[doc_id] for doc_id in serp_idf_2]\n",
    "rank_true_simple_3 = [true_relevance_3[doc_id] for doc_id in serp_simple_3]\n",
    "rank_true_idf_3 = [true_relevance_3[doc_id] for doc_id in serp_idf_3]\n",
    "\n",
    "rank_true_1 = [true_relevance_1[doc_id] for doc_id in true_relevance_1.keys()]\n",
    "rank_true_2 = [true_relevance_2[doc_id] for doc_id in true_relevance_2.keys()]\n",
    "rank_true_3 = [true_relevance_3[doc_id] for doc_id in true_relevance_3.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal NDCG for query 1: 3.13092975357\n",
      "Ideal NDCG for query 2: 6.67448909166\n",
      "Ideal NDCG for query 3: 6.08439426968\n"
     ]
    }
   ],
   "source": [
    "ideal_dcg_1 = ideal_dcg(rank_true_1)\n",
    "ideal_dcg_2 = ideal_dcg(rank_true_2)\n",
    "ideal_dcg_3 = ideal_dcg(rank_true_3)\n",
    "\n",
    "print \"Ideal NDCG for query 1:\", ideal_dcg_1\n",
    "print \"Ideal NDCG for query 2:\", ideal_dcg_2\n",
    "print \"Ideal NDCG for query 3:\", ideal_dcg_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print len(rank_true_simple_2)\n",
    "print len(serp_simple_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG for simple system, query 1: 0.0\n",
      "NDCG for simple system, query 2: 0.824676788397\n",
      "NDCG for simple system, query 3: 0.265678849989\n",
      "NDCG for system with idf, query 1: 0.859979711185\n",
      "NDCG for system with idf, query 2: 0.91706938669\n",
      "NDCG for system with idf, query 3: 0.605557277219\n"
     ]
    }
   ],
   "source": [
    "ndcg_simple_1 = ndcg(rank_true_simple_1, ideal_dcg_1)\n",
    "ndcg_idf_1 = ndcg(rank_true_idf_1, ideal_dcg_1)\n",
    "ndcg_simple_2 = ndcg(rank_true_simple_2, ideal_dcg_2)\n",
    "ndcg_idf_2 = ndcg(rank_true_idf_2, ideal_dcg_2)\n",
    "ndcg_simple_3 = ndcg(rank_true_simple_3, ideal_dcg_3)\n",
    "ndcg_idf_3 = ndcg(rank_true_idf_3, ideal_dcg_3)\n",
    "\n",
    "print \"NDCG for simple system, query 1:\", ndcg_simple_1\n",
    "print \"NDCG for simple system, query 2:\", ndcg_simple_2\n",
    "print \"NDCG for simple system, query 3:\", ndcg_simple_3\n",
    "\n",
    "print \"NDCG for system with idf, query 1:\", ndcg_idf_1\n",
    "print \"NDCG for system with idf, query 2:\", ndcg_idf_2\n",
    "print \"NDCG for system with idf, query 3:\", ndcg_idf_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_ndcg_simple = np.mean([ndcg_simple_1, ndcg_simple_2, ndcg_simple_3])\n",
    "mean_ndcg_idf = np.mean([ndcg_idf_1, ndcg_idf_2, ndcg_idf_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean NDCG for simple system: 0.363451879462\n",
      "Mean NDCG for system with idf: 0.794202125031\n"
     ]
    }
   ],
   "source": [
    "print \"Mean NDCG for simple system:\", mean_ndcg_simple\n",
    "print \"Mean NDCG for system with idf:\", mean_ndcg_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Языковая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_terms_probs(forward_index, token_list):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
