{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib\n",
    "import os\n",
    "import pymorphy2\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "RusLem = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def bprint(l, sep = \" \"):\n",
    "    print sep.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sentences_normaliation\n",
    "def normalization(collection):\n",
    "    for i, text in enumerate(collection):\n",
    "#         print text\n",
    "        tokens = re.findall('[\\w]+', text[0].strip().lower(), re.U)\n",
    "        text[0] = \" \".join([RusLem.parse(token)[0].normal_form for token in tokens])\n",
    "    return collection\n",
    "\n",
    "def get_tokens_probs(forward_index, token_list):\n",
    "    tokens_cnt = 0\n",
    "    token_probs = {token:0 for token in token_list}\n",
    "    for sent_idx in forward_index.keys():\n",
    "        for token in forward_index[sent_idx]:\n",
    "            token_probs[token] += 1\n",
    "        tokens_cnt += len(forward_index[sent_idx])\n",
    "        \n",
    "    token_probs = {token:token_probs[token] / float(tokens_cnt) for token in token_list}\n",
    "    return token_probs\n",
    "        \n",
    "    \n",
    "class NaiveBayesClassificator(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, collection_dir=\"./yandex_news/\"):\n",
    "        self.raw_collection = self.get_collection(collection_dir)\n",
    "        self.collection = normalization(deepcopy(self.raw_collection))\n",
    "        self.token_probs, self.class_probs, self.smoothing_values = self.get_tokens_probs(self.collection)\n",
    "        return self\n",
    "        \n",
    "    def get_collection(self, collection_dir):\n",
    "        \"\"\"\n",
    "        return list of pair [[text, class_id]\n",
    "        sport - class 0\n",
    "        policy - class 1\n",
    "        \"\"\"\n",
    "        collection = []\n",
    "        class_dirs = sorted(os.listdir(collection_dir))\n",
    "        cur_class_id = 0\n",
    "        for class_dir in class_dirs:\n",
    "            if class_dir == 'sport':\n",
    "                cur_class_id = 0\n",
    "            elif class_dir == 'policy':\n",
    "                cur_class_id = 1\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            files = os.listdir(os.path.join(collection_dir, class_dir))\n",
    "            \n",
    "            for filename in files:\n",
    "                with open(os.path.join(collection_dir, class_dir, filename)) as f:\n",
    "                    f_text = \" \".join(f.readlines()).decode('utf-8')\n",
    "                    collection.append([f_text, cur_class_id])\n",
    "\n",
    "        return collection\n",
    "    \n",
    "                      \n",
    "    def get_tokens_probs(self, collection):\n",
    "        \"\"\"\n",
    "        return {class_id:{token:token_freqs for this class}}\n",
    "        \"\"\"\n",
    "        token_freqs = {}\n",
    "        uniq_tokens = set()\n",
    "        class_probs = {}\n",
    "        for doc in self.collection:\n",
    "            if doc[1] not in token_freqs:\n",
    "                token_freqs[doc[1]] = {}\n",
    "            if doc[1] not in class_probs:\n",
    "                class_probs[doc[1]] = 0\n",
    "                      \n",
    "            class_probs[doc[1]] += 1\n",
    "            for token in doc[0].split(\" \"):\n",
    "#                 tokens = sent.split(\" \")\n",
    "                if token not in token_freqs[doc[1]]:\n",
    "                    token_freqs[doc[1]][token] = 0\n",
    "                    uniq_tokens.add(token)\n",
    "                token_freqs[doc[1]][token] += 1\n",
    "                      \n",
    "        class_probs = {class_id: class_probs[class_id] / float(len(self.collection))\n",
    "                       for class_id in class_probs.keys()}\n",
    "        \n",
    "        token_probs = {}\n",
    "        smoothing_values = {}\n",
    "        cnt_uniq_token = len(list(uniq_tokens))\n",
    "        for class_id in token_freqs.keys():\n",
    "            cnt_class_token = sum(token_freqs[class_id].values())\n",
    "            smoothing_values[class_id] = 1. /  float(cnt_class_token + cnt_uniq_token)\n",
    "            token_probs[class_id] = {token:\\\n",
    "                                     (token_freqs[class_id][token] + 1) / float(cnt_class_token + cnt_uniq_token)\n",
    "                                    for token in token_freqs[class_id].keys()}\n",
    "                      \n",
    "        return token_probs, class_probs, smoothing_values \n",
    "                      \n",
    "    def predict(self, doc):\n",
    "        prob_doc = {class_id:1 for class_id in self.token_probs}\n",
    "        doc_tokens = doc.split(\" \")\n",
    "        bprint(doc_tokens)\n",
    "        for class_id in prob_doc.keys():\n",
    "            prob_doc[class_id] = reduce(lambda x, y: x * y, [self.token_probs[class_id][token]\n",
    "                                                             if token in self.token_probs[class_id]\n",
    "                                                             else self.smoothing_values[class_id]\\\n",
    "                                                             for token in doc_tokens])\n",
    "            print \"Probability of class {}:{}\".format(class_id, prob_doc[class_id])\n",
    "        \n",
    "            \n",
    "\n",
    "def print_serp(rank, query, sent_cnt = 10):\n",
    "    sorted_rank = sorted(rank.items(), key = operator.itemgetter(1), reverse=True)\n",
    "    print \"Query:\",\n",
    "    bprint([query])\n",
    "    print \"\\n\"\n",
    "    serp = []\n",
    "    \n",
    "    for idx, pair in enumerate(sorted_rank):\n",
    "#         print idx, pair\n",
    "        if idx > sent_cnt:\n",
    "            break\n",
    "        print pair[1], \"\\t\", pair[0], \n",
    "        bprint(collection.forward_index[pair[0]])\n",
    "        print\n",
    "        serp.append(pair[0])\n",
    "    return serp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayesClassificator().train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "польский власть связать провокация кндр с план россия с польский точка зрение действие северный корея строго соотнести и связать с российский план и агрессивный поведение россия приводить газета ру слово польский министр\n"
     ]
    }
   ],
   "source": [
    "test_text_1 = u\"Польский власть связал провокации КНДР с «планами России\\\n",
    "                .«С польской точки зрения действия Северной Кореи строго соотнесены и\\\n",
    "                связаны с российскими планами и агрессивным поведением России», \\\n",
    "                — приводит «Газета.ру» слова польского министра.\"\n",
    "tokens = re.findall('[\\w]+', test_text_1.strip().lower(), re.U)\n",
    "test_text_1 = \" \".join([RusLem.parse(token)[0].normal_form for token in tokens])\n",
    "print test_text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "польский власть связать провокация кндр с план россия с польский точка зрение действие северный корея строго соотнести и связать с российский план и агрессивный поведение россия приводить газета ру слово польский министр\n",
      "Probability of class 0:5.47139928818e-91\n",
      "Probability of class 1:3.96828026559e-88\n"
     ]
    }
   ],
   "source": [
    "nb.predict(test_text_1)\n",
    "print \"Значит, данный текст относится к классу политики\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "анри мочь стать главный тренер сборная уэльс экс форвард арсенал тьерри анри мочь занять пост главное тренер сборная уэльс накануне уволить с данный место быть крис коулмэн перебраться в сандерленд анри на данный момент занимать должность помощник главное тренер сборная бельгия роберто мартинес\n"
     ]
    }
   ],
   "source": [
    "test_text_2 = u\"Анри может стать главным тренером сборной Уэльса. Экс-форвард «Арсенала» \\\n",
    "                Тьерри Анри может занять пост главного тренера сборной Уэльса. \\\n",
    "                Накануне уволен с данного места был Крис Коулмэн, перебравшийся в «Сандерленд». \\\n",
    "                Анри на данный момент занимает должность помощника главного тренера \\\n",
    "                сборной Бельгии Роберто Мартинеса.\"\n",
    "tokens = re.findall('[\\w]+', test_text_2.strip().lower(), re.U)\n",
    "test_text_2 = \" \".join([RusLem.parse(token)[0].normal_form for token in tokens])\n",
    "print test_text_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "анри мочь стать главный тренер сборная уэльс экс форвард арсенал тьерри анри мочь занять пост главное тренер сборная уэльс накануне уволить с данный место быть крис коулмэн перебраться в сандерленд анри на данный момент занимать должность помощник главное тренер сборная бельгия роберто мартинес\n",
      "Probability of class 0:4.61873125317e-122\n",
      "Probability of class 1:7.49450887933e-127\n",
      "Значит, данный текст относится к классу спорт\n"
     ]
    }
   ],
   "source": [
    "nb.predict(test_text_2)\n",
    "print \"Значит, данный текст относится к классу спорт\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
