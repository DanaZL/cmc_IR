{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import etree       \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import pymorphy2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import re\n",
    "import cPickle as pickle\n",
    "import os\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import io\n",
    "import scipy\n",
    "import sys\n",
    "import gc\n",
    "import random as r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Преобразовываем xml в csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = etree.parse('./news_sentiment_romip2012/train/news_eval_train.xml')  \n",
    "root = tree.getroot() \n",
    "corpus = root\n",
    "data = []\n",
    "\n",
    "for sentense in corpus:\n",
    "    new_sentense = []\n",
    "    new_sentense.append(sentense.items()[0][1])\n",
    "#     print sentense.getchildren()\n",
    "    for field in sentense[:-1]:\n",
    "        if field.text is None:\n",
    "            new_sentense.append(None)\n",
    "        else:\n",
    "            \n",
    "            new_sentense.append(field.text.encode('utf-8').strip())\n",
    "    data.append(new_sentense)\n",
    "\n",
    "Sentences = pd.DataFrame(np.asarray(data), columns = ['id', 'text', 'label'])\n",
    "Sentences[['id']] = Sentences[['id']].apply(pd.to_numeric) \n",
    "Sentences = Sentences[(Sentences.label == \"0\") | (Sentences.label == \"+\") | (Sentences.label == \"-\")]\n",
    "Sentences['label'] = Sentences['label'].map({'0':1, '-':0, '+':2})\n",
    "Sentences.to_csv(\"./train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = etree.parse('./news_sentiment_romip2012/test/news_eval_test.xml')  \n",
    "root = tree.getroot() \n",
    "corpus = root\n",
    "data = []\n",
    "\n",
    "for sentense in corpus:\n",
    "    new_sentense = []\n",
    "    new_sentense.append(sentense.items()[0][1])\n",
    "#     print sentense.getchildren()\n",
    "    for field in sentense[:-1]:\n",
    "        if field.text is None:\n",
    "            new_sentense.append(None)\n",
    "        else:\n",
    "            \n",
    "            new_sentense.append(field.text.encode('utf-8').strip())\n",
    "    data.append(new_sentense)\n",
    "\n",
    "Sentences = pd.DataFrame(np.asarray(data), columns = ['id', 'text', 'label'])\n",
    "Sentences[['id']] = Sentences[['id']].apply(pd.to_numeric) \n",
    "Sentences = Sentences[(Sentences.label == \"0\") | (Sentences.label == \"+\") | (Sentences.label == \"-\")]\n",
    "Sentences['label'] = Sentences['label'].map({'0':1, '-':0, '+':2})\n",
    "Sentences.to_csv(\"./test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Нормализуем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RusLem = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3893it [01:03, 65.37it/s]\n"
     ]
    }
   ],
   "source": [
    "Sentences = pd.read_csv(\"./train.csv\")\n",
    "np_sentences = Sentences.values\n",
    "\n",
    "for sent_idx, sent in tqdm(enumerate(np_sentences)):\n",
    "    tokens = re.findall('[\\w]+',sent[1].decode(\"utf-8\").strip().lower(), re.U)\n",
    "    tokens = [RusLem.parse(token)[0].normal_form for token in tokens]\n",
    "    np_sentences[sent_idx][1] = \" \".join(tokens)\n",
    "    \n",
    "with open(\"./train_norm\", 'w') as res_file:\n",
    "    pickle.dump(np_sentences, res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4573it [01:17, 42.84it/s]\n"
     ]
    }
   ],
   "source": [
    "Sentences = pd.read_csv(\"./test.csv\")\n",
    "np_sentences = Sentences.values\n",
    "\n",
    "for sent_idx, sent in tqdm(enumerate(np_sentences)):\n",
    "    tokens = re.findall('[\\w]+',sent[1].decode(\"utf-8\").strip().lower(), re.U)\n",
    "    tokens = [RusLem.parse(token)[0].normal_form for token in tokens]\n",
    "    np_sentences[sent_idx][1] = \" \".join(tokens)\n",
    "    \n",
    "with open(\"./test_norm\", 'w') as res_file:\n",
    "    pickle.dump(np_sentences, res_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создаем 3 векторных представления -  бинарные вектора, частоты и tf.idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3893 4573\n"
     ]
    }
   ],
   "source": [
    "with open(\"./train_norm\", 'r') as res_file:\n",
    "    np_sentences_train = pickle.load(res_file)\n",
    "with open(\"./test_norm\", 'r') as res_file:\n",
    "    np_sentences_test = pickle.load(res_file)\n",
    "    \n",
    "print len(np_sentences_train), len(np_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3893 4573 3893 4573\n"
     ]
    }
   ],
   "source": [
    "train_text = [item[1] for item in np_sentences_train]\n",
    "train_label = [int(item[2]) for item in np_sentences_train]\n",
    "test_text = [item[1] for item in np_sentences_test]\n",
    "test_label = [int(item[2]) for item in np_sentences_test]\n",
    "print len(train_text), len(test_text),  len(train_label), len(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binary_vectorizer = CountVectorizer(binary=True)\n",
    "train_binary = binary_vectorizer.fit_transform(train_text).toarray()\n",
    "test_binary = binary_vectorizer.transform(test_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(binary=False)\n",
    "train_freq = count_vectorizer.fit_transform(train_text).toarray()\n",
    "test_freq = count_vectorizer.transform(test_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train_text).toarray()\n",
    "test_tfidf = tfidf_vectorizer.transform(test_text).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear]"
     ]
    }
   ],
   "source": [
    "svm_binary = LinearSVC(max_iter=100000, verbose=2).fit(train_binary, train_label)\n",
    "svm_freq = LinearSVC(max_iter=100000, verbose=1).fit(train_freq, train_label)\n",
    "svm_tfidf = LinearSVC(max_iter=100000, verbose=1).fit(train_tfidf, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_binary = svm_binary.predict(test_binary)\n",
    "predict_freq = svm_freq.predict(test_freq)\n",
    "predict_tfidf = svm_tfidf.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 0 1 2 0 2 0 0 0]\n",
      "[2 1 1 1 2 0 2 0 0 0]\n",
      "[2 0 1 0 0 0 2 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print predict_binary[:10]\n",
    "print predict_freq[:10]\n",
    "print predict_tfidf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F-measure\n",
      "Бинарные вектора:  0.575844001209\n",
      "Частотные вектора:  0.576135308447\n",
      "Tf-Idf вектора:  0.597736897068\n",
      "\n",
      "Micro F-measure\n",
      "Бинарные вектора:  0.579269626066\n",
      "Частотные вектора:  0.579706975727\n",
      "Tf-Idf вектора:  0.612726875137\n",
      "\n",
      "Macro F-measure\n",
      "Бинарные вектора:  0.559447225885\n",
      "Частотные вектора:  0.559523708572\n",
      "Tf-Idf вектора:  0.575986211597\n"
     ]
    }
   ],
   "source": [
    "print \"Weighted F-measure\"\n",
    "print \"Бинарные вектора: \", f1_score(test_label, predict_binary, average='weighted')\n",
    "print \"Частотные вектора: \", f1_score(test_label, predict_freq, average='weighted')\n",
    "print \"Tf-Idf вектора: \", f1_score(test_label, predict_tfidf, average='weighted')\n",
    "print\n",
    "print \"Micro F-measure\"\n",
    "print \"Бинарные вектора: \", f1_score(test_label, predict_binary, average='micro')\n",
    "print \"Частотные вектора: \", f1_score(test_label, predict_freq, average='micro')\n",
    "print \"Tf-Idf вектора: \", f1_score(test_label, predict_tfidf, average='micro')\n",
    "print \n",
    "print \"Macro F-measure\"\n",
    "print \"Бинарные вектора: \", f1_score(test_label, predict_binary, average='macro')\n",
    "print \"Частотные вектора: \", f1_score(test_label, predict_freq, average='macro')\n",
    "print \"Tf-Idf вектора: \", f1_score(test_label, predict_tfidf, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb_binary = BernoulliNB(binarize=None).fit(train_binary, train_label)\n",
    "mb_freq = MultinomialNB().fit(train_freq, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_binary = mb_binary.predict(test_binary)\n",
    "predict_freq = mb_freq.predict(test_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 0 0 0 0 2 0 0 2 2 2 0 2 2 0 0 0 0 0 0 1 2 0 0 2 0 0 1 2]\n",
      "[2 0 0 0 0 0 2 0 0 2 2 2 2 2 2 0 0 2 0 0 2 1 2 0 1 2 0 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print predict_binary[:30]\n",
    "print predict_freq[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F-measure\n",
      "Бинарные вектора (Bernoulli Naive Bayes):  0.556684297921\n",
      "Частотные вектора (Multinomial Naive Bayes):  0.608023069625\n",
      "\n",
      "Micro F-measure\n",
      "Бинарные вектора (Bernoulli Naive Bayes):  0.599387710475\n",
      "Частотные вектора (Multinomial Naive Bayes):  0.631751585393\n",
      "\n",
      "Macro F-measure\n",
      "Бинарные вектора (Bernoulli Naive Bayes):  0.528240378265\n",
      "Частотные вектора (Multinomial Naive Bayes):  0.584514362325\n"
     ]
    }
   ],
   "source": [
    "print \"Weighted F-measure\"\n",
    "print \"Бинарные вектора (Bernoulli Naive Bayes): \", f1_score(test_label, predict_binary, average='weighted')\n",
    "print \"Частотные вектора (Multinomial Naive Bayes): \", f1_score(test_label, predict_freq, average='weighted')\n",
    "print\n",
    "print \"Micro F-measure\"\n",
    "print \"Бинарные вектора (Bernoulli Naive Bayes): \", f1_score(test_label, predict_binary, average='micro')\n",
    "print \"Частотные вектора (Multinomial Naive Bayes): \", f1_score(test_label, predict_freq, average='micro')\n",
    "print \n",
    "print \"Macro F-measure\"\n",
    "print \"Бинарные вектора (Bernoulli Naive Bayes): \", f1_score(test_label, predict_binary, average='macro')\n",
    "print \"Частотные вектора (Multinomial Naive Bayes): \", f1_score(test_label, predict_freq, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.96027335\n",
      "Validation score: 0.638462\n",
      "Iteration 2, loss = 0.38334274\n",
      "Validation score: 0.717949\n",
      "Iteration 3, loss = 0.06398220\n",
      "Validation score: 0.707692\n",
      "Iteration 4, loss = 0.01790800\n",
      "Validation score: 0.710256\n",
      "Iteration 5, loss = 0.01138526\n",
      "Validation score: 0.707692\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlp_binary = MLPClassifier(hidden_layer_sizes=(1000, 200), \n",
    "                           early_stopping=True, max_iter=1000000, verbose=1).fit(train_binary, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.00165238\n",
      "Validation score: 0.589744\n",
      "Iteration 2, loss = 0.54291951\n",
      "Validation score: 0.674359\n",
      "Iteration 3, loss = 0.12094150\n",
      "Validation score: 0.671795\n",
      "Iteration 4, loss = 0.01908670\n",
      "Validation score: 0.661538\n",
      "Iteration 5, loss = 0.01116698\n",
      "Validation score: 0.653846\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlp_tfidf = MLPClassifier(hidden_layer_sizes=(1000, 200), \n",
    "                        early_stopping=True, max_iter=1000000, verbose=1).fit(train_tfidf, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_binary = mlp_binary.predict(test_binary)\n",
    "predict_tfidf = mlp_tfidf.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 0 0 0 0 2 0 0 0 2 2 2 2 2 0 2 0 0 0 0 1 2 0 1 2 0 0 1 2]\n",
      "[2 0 1 0 0 0 2 0 0 2 2 2 2 2 2 0 2 0 0 0 2 1 2 0 1 2 0 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print predict_binary[:30]\n",
    "print predict_tfidf[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F-measure\n",
      "Бинарные вектора:  0.61959939841\n",
      "Tf-Idf вектора:  0.609983861535\n",
      "\n",
      "Micro F-measure\n",
      "Бинарные вектора:  0.63065821124\n",
      "Tf-idf вектора:  0.631970260223\n",
      "\n",
      "Macro F-measure\n",
      "Бинарные вектора:  0.600634310491\n",
      "Tf-idf вектора:  0.586892818653\n"
     ]
    }
   ],
   "source": [
    "print \"Weighted F-measure\"\n",
    "print \"Бинарные вектора: \", f1_score(test_label, predict_binary, average='weighted')\n",
    "print \"Tf-Idf вектора: \", f1_score(test_label, predict_tfidf, average='weighted')\n",
    "print\n",
    "print \"Micro F-measure\"\n",
    "print \"Бинарные вектора: \", f1_score(test_label, predict_binary, average='micro')\n",
    "print \"Tf-idf вектора: \", f1_score(test_label, predict_tfidf, average='micro')\n",
    "print \n",
    "print \"Macro F-measure\"\n",
    "print \"Бинарные вектора: \", f1_score(test_label, predict_binary, average='macro')\n",
    "print \"Tf-idf вектора: \", f1_score(test_label, predict_tfidf, average='macro')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
